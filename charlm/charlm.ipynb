{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ffc3bf1-a6f6-46d3-ae31-a71796c7ec9f",
   "metadata": {},
   "source": [
    "# Assignment 4, task 3\n",
    "\n",
    "In this task, you are going to implement a character model based on the Transformer architecture, starting from the provided skeleton. It is useful to first do exercise 2 before starting on this task.\n",
    "\n",
    "The model you are going to implement here will have a context of 32 characters, i.e. it will consider the preceding 32 characters when estimating the probabilities of the possible character coming next. Due to the clever transformer architecture, the model will have less than 50,000 trainable parameters. As a comparison, the simpler model in exercise 2 only had a context of 8 characters but had more than 300,000 trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714fe121-b62c-4590-98dd-13eceab39989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First run this cell\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datetime import datetime\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca690c4-0e0c-4abb-b53b-c19c38f980be",
   "metadata": {},
   "source": [
    "The __self-attention__ computation is at the core of the Transformer architecture. It is important to get this computation efficient (i.e. vectorized), since it involves many matrix operations that would be very slow if implemented by Python loops.\n",
    "\n",
    "The input to the self-attention computation is a tensor containing a vector for each input token, and the output is a tensor of the same dimensions, containing the contextualized versions of the input tokens (see Lecture 9 and the textbook, chapters 10.1 and 10.2).\n",
    "\n",
    "Your task is to fill in the missing pieces below. Look for \"REPLACE WITH YOUR CODE\" and \"YOUR CODE HERE\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38dfe02-5e56-4214-bd2d-6cb6e4b48c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Computes self-attention according to Vashwani et al, 2017.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vector_dim, att_dim):\n",
    "        \"\"\"\n",
    "        vector_dim = the dimension of the input and output vectors\n",
    "        att_dim = the (usually) smaller dimension use in the attention \n",
    "                  computation\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.vector_dim = vector_dim\n",
    "        self.att_size = att_dim\n",
    "        self.wq = nn.Linear(vector_dim, att_dim, bias=False)\n",
    "        self.wk = nn.Linear(vector_dim, att_dim, bias=False)\n",
    "        self.wv = nn.Linear(vector_dim, att_dim, bias=False)\n",
    "        self.wo = nn.Linear(att_dim, vector_dim, bias=False)\n",
    "        self.method = \"single-head attention\"\n",
    "\n",
    "    def compute_attention(self, q, k, v):\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = self.wq(x)\n",
    "        k = self.wk(x)\n",
    "        v = self.wv(x)\n",
    "        values = self.compute_attention(q, k, v)\n",
    "        out = self.wo(values)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f83691b-59a1-40f0-ab10-387b24569893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the code\n",
    "seed = 4224\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # For multi-GPU setups\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "batch_size = 1\n",
    "seq_len = 20\n",
    "vector_dim = 64\n",
    "att_dim = 32\n",
    "attention = SelfAttention(vector_dim, att_dim)\n",
    "data = torch.rand(batch_size, seq_len, vector_dim)\n",
    "data[0][-1] = torch.zeros(vector_dim)\n",
    "result = attention(data)\n",
    "print (\"Sample some results using\", attention.method )\n",
    "print(f'{result[0][7][7].detach().item():.4f}' == '-0.3068')\n",
    "print(f'{result[0][8][8].detach().item():.4f}' == '0.1224')\n",
    "print(f'{result[0][9][9].detach().item():.4f}' == '-0.3258')\n",
    "print(f'{result[0][-1][9].detach().item():.4f}' == '0.0000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4b024e-02d5-49d6-b0e2-f58f7924259e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Computes self-attention according to Vashwani et al, 2017.\n",
    "    Second version: with multiple heads\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vector_dim, att_dim):\n",
    "        \"\"\"\n",
    "        vector_dim = the dimension of the input and output vectors\n",
    "        att_dim = the (usually) smaller dimension use in the attention \n",
    "                  computation.\n",
    "\n",
    "        NOTE that vector_dim must be a multiple of att_dim.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert vector_dim % att_dim == 0\n",
    "        self.no_of_heads = vector_dim // att_dim\n",
    "        self.vector_dim = vector_dim\n",
    "        self.att_dim = att_dim\n",
    "        self.wq = nn.Linear(att_dim, att_dim, bias=False)\n",
    "        self.wk = nn.Linear(att_dim, att_dim, bias=False)\n",
    "        self.wv = nn.Linear(att_dim, att_dim, bias=False)\n",
    "        self.wo = nn.Linear(vector_dim, vector_dim, bias=False)\n",
    "        self.method = \"multi-head attention\"\n",
    "\n",
    "    def compute_attention(self, q, k, v):\n",
    "        \n",
    "        # COPY YOUR CODE FROM ABOVE\n",
    "\n",
    "\n",
    "    def reshape_for_multihead_attention(self, x):\n",
    "        \"\"\"\n",
    "        x has the shape (batch_size, seq_length, vector_dim)\n",
    "\n",
    "        We want to split the representation of each token into 'no_of_heads'\n",
    "        parts and treat each part separately. Thus, we need the returned tensor\n",
    "        to have shape (batch_size, no_of_heads, seq_length, att_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "\n",
    "\n",
    "    def reshape_after_multihead_attention(self, x):\n",
    "        \"\"\"\n",
    "        x has the shape (batch_size, no_of_heads, seq_length, att_dim)\n",
    "\n",
    "        For each token, we now want to bring together the representation coming\n",
    "        from each head. The returned token should have the shape:\n",
    "        (batch_size, seq_length, vector_dim)\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "       \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.reshape_for_multihead_attention(x)\n",
    "        q = self.wq(x)\n",
    "        k = self.wk(x)\n",
    "        v = self.wv(x)\n",
    "        values = self.compute_attention(q, k, v)\n",
    "        values = self.reshape_after_multihead_attention(values)\n",
    "        out = self.wo(values)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9072ad5-2518-45df-b79e-8d07087652ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test the SelfAttention class. Make sure that you first run the cell above!\n",
    "\n",
    "seed = 4224\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # For multi-GPU setups\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "batch_size = 1\n",
    "seq_len = 20\n",
    "vector_dim = 64\n",
    "att_dim = 32\n",
    "attention = SelfAttention(vector_dim, att_dim)\n",
    "data = torch.rand(batch_size, seq_len, vector_dim)\n",
    "data[0][-1] = torch.zeros(vector_dim)\n",
    "print(\"Check that reshaping works:\")\n",
    "try:\n",
    "    desired_shape = torch.Size([batch_size, attention.no_of_heads, seq_len, att_dim])\n",
    "    reshaped_data = attention.reshape_for_multihead_attention(data)\n",
    "    print( reshaped_data.shape == desired_shape )\n",
    "    print( (data[0][9][33] == reshaped_data[0][1][9][1]).detach().item() )\n",
    "    rereshaped_data = attention.reshape_after_multihead_attention(reshaped_data)\n",
    "    print( data.shape == rereshaped_data.shape )\n",
    "    print( torch.all(data == rereshaped_data).detach().item() )\n",
    "except AttributeError:\n",
    "    print (\"WARNING: Something went wrong. Perhaps you forgot to run the cell above this one?\")\n",
    "result = attention(data)\n",
    "print (\"Sample some results using\", attention.method )\n",
    "print(f'{result[0][7][7].detach().item():.4f}' == '0.0013')\n",
    "print(f'{result[0][8][8].detach().item():.4f}' == '0.0265')\n",
    "print(f'{result[0][9][9].detach().item():.4f}' == '-0.0596')\n",
    "print(f'{result[0][-1][9].detach().item():.4f}' == '0.0000')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d04a631-1537-46fa-a717-7c38cdb5af14",
   "metadata": {},
   "source": [
    "We need to map every type of input item (every character, in our case) to a unique ID number. Since we are not sure which characters will appear in our training text, we are going to create new IDs as we encounter new kinds of characters we haven't seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f2552b-d725-46b5-b828-763bb7f4e0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_id = {}  # Dictionary to store character-to-ID mapping\n",
    "id_to_char = []  # List to store characters in their ID ordering\n",
    "PADDING_SYMBOL = '<PAD>'\n",
    "char_to_id[PADDING_SYMBOL] = 0 \n",
    "id_to_char.append( PADDING_SYMBOL )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a38c7ae-14c2-4493-a90e-e743b2801b65",
   "metadata": {},
   "source": [
    "We now define a class 'CharDataset' that extends the predefined 'Dataset' class.Compared to exercise 2, we will create data points in a slightly different way. \n",
    "\n",
    "The init function reads a training text and splits it up into chunks $n$ characters long. From each chunk, $n$ data points with a corresponding label will be created, as in the following example:\n",
    "\n",
    "Suppose $n=8$. From a chunk $[4,5,9,11,7,7,2,12]$ with 14 being the next character ID, the following data points and labels will be formed (0 is the padding symbol):\n",
    "\n",
    "| Data point | Label |\n",
    "|-----------:|------:|\n",
    "|[4,0,0,0,0,0,0,0] | 5 |\n",
    "|[4,5,0,0,0,0,0,0] | 9 |\n",
    "|[4,5,9,0,0,0,0,0] | 11 |\n",
    "|[4,5,9,11,0,0,0,0] | 7 |\n",
    "|[4,5,9,11,7,0,0,0] | 7 |\n",
    "|[4,5,9,11,7,7,0,0] | 2 |\n",
    "|[4,5,9,11,7,7,2,0] | 12 |\n",
    "|[4,5,9,11,7,7,2,12] | 14 |\n",
    "\n",
    "This way, the model will learn to infer the next character even if the context is shorter than $n$. This is a very useful feature, particularly in 'real' language models, where the known context often is shorter than the maximal context length.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cca040d-2d81-4081-9f98-03762a997282",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset) :\n",
    "\n",
    "    def __init__(self, file_path, n) :\n",
    "        self.datapoints = []\n",
    "        self.labels = []\n",
    "        chars = []\n",
    "        try :\n",
    "            # First read the dataset to find all the unique characters\n",
    "            with open(file_path,'r',encoding='utf-8') as f :\n",
    "                contents = f.read()\n",
    "            for char in contents:\n",
    "                if char not in char_to_id:\n",
    "                    char_to_id[char] = len(id_to_char)\n",
    "                    id_to_char.append(char)\n",
    "                chars.append( char_to_id[char] )\n",
    "            # Then go through all chars list and create a list of datapoints\n",
    "            k = 0\n",
    "            while k < len(chars)-n:\n",
    "                for i in range(1, n+1):\n",
    "                    self.datapoints.append([c for c in chars[k:i+k]+[0]*(n-i)])\n",
    "                    self.labels.append(chars[i+k])\n",
    "                k += n\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "    def __len__(self) :\n",
    "        return len(self.datapoints)\n",
    "\n",
    "    def __getitem__(self, idx) :\n",
    "        idx = idx % len(self.datapoints)\n",
    "        return torch.tensor(self.datapoints[idx]), torch.tensor(self.labels[idx], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a7e7c5-4b93-4d6c-b725-156ec8b29ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFFN(nn.Module):\n",
    "    \"\"\"\n",
    "    The position-wise FFN that follows after the self-attention\n",
    "    computation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size, dropout_prob) :\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size, bias=True)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size, bias=True)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.dropout(torch.relu(self.fc1(x))))\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer encoder block.\n",
    "\n",
    "    This version differs from the original version in  [Vaswani et al. NeurIPS 2017],\n",
    "    and applies the LayerNorm before the self-attention, and before the FFN, as this\n",
    "    has proved to be beneficial (see [Nguyen and Salazar 2019]).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vector_dim, att_dim, dropout_prob):\n",
    "        super().__init__()\n",
    "        self.attn = SelfAttention(vector_dim, att_dim)\n",
    "        self.ffn = PositionwiseFFN(vector_dim, dropout_prob)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.ln1 = nn.LayerNorm(vector_dim)\n",
    "        self.ln2 = nn.LayerNorm(vector_dim)\n",
    "        self.attention_method = self.attn.method\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.ln1(x)\n",
    "        x2 = x + self.dropout(self.attn(x1))\n",
    "        x3 = self.ln2(x2)\n",
    "        x4 = x2 + self.dropout(self.ffn(x3))\n",
    "        return x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702ecae4-3063-403b-bae8-8f10be240805",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFFN(nn.Module):\n",
    "    \"\"\"\n",
    "    The position-wise FFN that follows after the self-attention\n",
    "    computation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size, dropout_prob) :\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size, bias=True)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size, bias=True)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        # for module in (self.fc1, self.fc2):\n",
    "        #     nn.init.kaiming_normal_(module.weight)\n",
    "        #     nn.init.constant_(module.bias, 0.)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.dropout(torch.relu(self.fc1(x))))\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer encoder block.\n",
    "\n",
    "    This version differs from the original version in  [Vaswani et al. NeurIPS 2017],\n",
    "    and applies the LayerNorm before the self-attention, and before the FFN, as this\n",
    "    has proved to be beneficial (see [Nguyen and Salazar 2019]).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vector_dim, att_dim, dropout_prob):\n",
    "        super().__init__()\n",
    "        self.attn = SelfAttention(vector_dim, att_dim)\n",
    "        self.ffn = PositionwiseFFN(vector_dim, dropout_prob)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.ln1 = nn.LayerNorm(vector_dim)\n",
    "        self.ln2 = nn.LayerNorm(vector_dim)\n",
    "        self.attention_method = self.attn.method\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.ln1(x)\n",
    "        x2 = x + self.dropout(self.attn(x1))\n",
    "        x3 = self.ln2(x2)\n",
    "        x4 = x2 + self.dropout(self.ffn(x3))\n",
    "        return x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fd41d6-7c9e-434a-88d0-0349e9e4243d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ======================= Training ======================= #\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print( \"Running on\", device )\n",
    "\n",
    "config = Config()\n",
    "training_dataset = CharDataset('HP_book_1.txt', MAXLEN)\n",
    "print( \"There are\", len(training_dataset), \"datapoints and\", len(id_to_char), \"unique characters in the dataset\" ) \n",
    "training_loader = DataLoader(training_dataset, batch_size=config.batch_size)\n",
    "\n",
    "charlm = CharLM( config, len(id_to_char)).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "charlm_optimizer = optim.Adam( charlm.parameters(), lr=config.learning_rate )\n",
    "\n",
    "print( \"Using\", charlm.attention_method )\n",
    "print( datetime.now().strftime(\"%X\"), \"Training starts\" )\n",
    "for epoch in range(config.no_of_epochs) :\n",
    "    charlm.train()\n",
    "    iteration = 0\n",
    "    loss_sum = 0\n",
    "    for input_tensor, label in training_loader :\n",
    "        input_tensor, label = input_tensor.to(device), label.to(device)\n",
    "        charlm_optimizer.zero_grad()\n",
    "        logits = charlm(input_tensor).to(device)\n",
    "        loss = criterion(logits.squeeze(1), label)\n",
    "        loss.backward()\n",
    "        charlm_optimizer.step()\n",
    "        iteration += 1\n",
    "        loss_sum += loss.detach().item()\n",
    "\n",
    "    print( datetime.now().strftime(\"%X\"), \"End of epoch\", epoch+1, \", average loss=\", loss_sum/iteration)\n",
    "    charlm.eval()\n",
    "    # Generate some characters starting from the input text\n",
    "    try :\n",
    "        char_list = list(\"he looked around and\"[-MAXLEN:])\n",
    "        for i in range(300) :\n",
    "            input_tensor = torch.tensor( [char_to_id[c] for c in char_list] + [char_to_id[PADDING_SYMBOL]]*(MAXLEN-len(char_list))).unsqueeze(0).to(device)\n",
    "            logits = charlm(input_tensor).squeeze().to(device)\n",
    "            _, new_character_tensor = logits.topk(1)\n",
    "            new_character = id_to_char[new_character_tensor.detach().item()]\n",
    "            print( new_character, end='' )\n",
    "            if len(char_list) == MAXLEN :\n",
    "                char_list.pop(0)\n",
    "            char_list.append( new_character )\n",
    "        print()\n",
    "    except KeyError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112a1123-4f5f-4f62-aedc-4eab451f52f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== User interaction ==================== #\n",
    "\n",
    "while True:\n",
    "    text = input(\"> \").strip()\n",
    "    if text == \"\" :\n",
    "        continue\n",
    "    char_list = list(text[-MAXLEN:])\n",
    "    # Generate 50 characters starting from the input text\n",
    "    try :\n",
    "        for i in range(50) :\n",
    "            input_tensor = torch.tensor( [char_to_id[c] for c in char_list] + [char_to_id[PADDING_SYMBOL]]*(MAXLEN-len(char_list))).unsqueeze(0).to(device)\n",
    "            logits = charlm(input_tensor).squeeze().to(device)\n",
    "            #dist = torch.distributions.categorical.Categorical(logits=logits)\n",
    "            _, new_character_tensor = logits.topk(1)\n",
    "            new_character = id_to_char[new_character_tensor.detach().item()]\n",
    "            #new_character = id_to_char[dist.sample().detach().item()]\n",
    "            print( new_character, end='' )\n",
    "            if len(char_list) == MAXLEN :\n",
    "                char_list.pop(0)\n",
    "            char_list.append( new_character )\n",
    "        print()\n",
    "    except KeyError :\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
